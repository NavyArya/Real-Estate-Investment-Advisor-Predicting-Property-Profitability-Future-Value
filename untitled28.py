# -*- coding: utf-8 -*-
"""Untitled28.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GQ44pXvWaH6hTPLgaiyhU1o9adEGqsIU

# PROJECT NAME- Real Estate Investment Advisor: Predicting Property Profitability & Future Value

# PROJECT SUMMARY- This project builds an end‑to‑end real estate investment advisor that uses machine learning to help users decide whether a property is a good investment and what its price might be after 5 years. It combines data preprocessing, EDA, modeling, MLOps, and deployment into one full workflow.​

Objective and scope
Classification task: predict a binary label “Good Investment” for each property using features like location, property type, size, price, amenities, and derived scores.​

Regression task: predict Future_Price_5Y using current price, property characteristics, and investment-related features (e.g., price per sqft, infrastructure).​

Data and feature engineering
Dataset: Indian housing data with columns such as State, City, Locality, Property_Type, BHK, Size_in_SqFt, Price_in_Lakhs, Year_Built, Furnished_Status, Nearby_Schools, Nearby_Hospitals, PublicTransportAccessibility, Parking_Space, Security, Amenities, Owner_Type, and Availability_Status.​

Engineered features: Price_per_SqFt, School density/infrastructure scores, Age_of_Property, and rule-based “Good Investment” labels using price vs median, price per sqft vs median, and multi-factor conditions like BHK≥3, RERA, and ready-to-move.​

Analysis and insights
EDA questions: price and size distributions, price per sqft by property type, state/city/locality-level price patterns, BHK distribution, and trends for top expensive localities.​

Relationship analysis: correlations among numeric features, effect of schools and hospitals on price per sqft, impact of furnished status, facing direction, amenities, parking, and public transport on value and investment potential.​

Modeling and evaluation
Classification models: Logistic Regression, Random Forest, XGBoost (or similar) trained on Good_Investment target with metrics Accuracy, Precision, Recall, F1, ROC-AUC.​

Regression models: Linear Regression, Random Forest Regressor, XGBoost Regressor trained on Future_Price_5Y with RMSE, MAE, and R² as evaluation metrics, aiming for high accuracy and low error in price forecasts.​

MLflow and Streamlit deployment
MLflow: used to track multiple experiments, log parameters, metrics, and artifacts, and register the best classification and regression models for reuse and deployment.​​

Streamlit app: provides a user-friendly form to input property details and filters; displays “Is this a Good Investment?” plus “Estimated Price after 5 Years,” along with visual insights such as city-wise trends, heatmaps, and feature importance for transparency

#  GITHUB LINK- https://github.com/NavyArya/Real-Estate-Investment-Advisor-Predicting-Property-Profitability-Future-Value.git

# PROBLEM STATEMENT- Develop a machine learning application to assist potential investors in making real estate decisions. The system should:
1.	Classify whether a property is a "Good Investment" (Classification).
2.	Predict the estimated property price after 5 years (Regression).
Use the provided dataset to preprocess and analyze the data, engineer relevant features, and deploy a user-interactive application using Streamlit that provides investment recommendations and price forecasts. MLflow will be used for experiment tracking.

# lets begin-
"""

import pandas as pd
import numpy as np

# load dataset
from google.colab import files
uploaded = files.upload()

import io
# Check missing values and duplicates
df = pd.read_csv(io.BytesIO(uploaded['india_housing_prices.csv']))
print("Missing values per column:\n", df.isnull().sum())
print("Number of duplicate rows:", df.duplicated().sum())

#Drop exact duplicate rows
df = df.drop_duplicates().reset_index(drop=True)
print("Shape after dropping duplicates:", df.shape)

#Separate numeric and categorical columns
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()
print("Numeric columns:", num_cols)
print("Categorical columns:", cat_cols)

for col in num_cols:
    median_value = df[col].median()
    df[col] = df[col].fillna(median_value)

for col in cat_cols:
    mode_value = df[col].mode()[0] # Access the first mode if multiple exist
    df[col] = df[col].fillna(mode_value)

#Verify no missing values remain
print("Total missing values after imputation:", df.isnull().sum().sum())

from sklearn.preprocessing import StandardScaler

# Select numeric columns to scale (edit this list to match your file)
num_features = [
"Size_in_SqFt",
"Price_in_Lakhs",
"Age_of_Property",
"Nearby_Schools",
"Nearby_Hospitals"
]

# Keep only columns that exist
num_features = [col for col in num_features if col in df.columns]
print("Numeric features to scale:", num_features)

# Scale
scaler = StandardScaler()
df[num_features] = scaler.fit_transform(df[num_features])

print(df[num_features].head())

from sklearn.preprocessing import OneHotEncoder

# Choose categorical columns (update based on your dataset)
cat_features = [
"State",
"City",
"Locality",
"Property_Type",
"Furnished_Status",
"Availability_Status"
]

# Keep only columns that exist
cat_features = [col for col in cat_features if col in df.columns]
print("Categorical features to encode:", cat_features);

# One-hot encode
encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
encoded_array = encoder.fit_transform(df[cat_features])
encoded_cols = encoder.get_feature_names_out(cat_features);

encoded_df = pd.DataFrame(encoded_array, columns=encoded_cols, index=df.index);

# Drop original categorical columns and concatenate encoded
df_encoded = pd.concat([df.drop(columns=cat_features), encoded_df], axis=1);

print("Shape after encoding:", df_encoded.shape)

# Price per SqFt
if "Price_in_Lakhs" in df.columns and "Size_in_SqFt" in df.columns:
    # Convert lakhs to full currency and divide by sqft
    df["Price_per_SqFt"] = (
        df["Price_in_Lakhs"] * 100000 /
        df["Size_in_SqFt"].replace(0, np.nan)
    )
    df["Price_per_SqFt"] = df["Price_per_SqFt"].replace([np.inf, -np.inf], np.nan)
    df["Price_per_SqFt"] = df["Price_per_SqFt"].fillna(df["Price_per_SqFt"].median())
    print("Price_per_SqFt created.")
else:
    print("Price_in_Lakhs or Size_in_SqFt not found. Check column names.")

# 3.2 School Density Score (0-1 normalized)
if "Nearby_Schools" in df.columns:
    schools = df["Nearby_Schools"]
    df["School_Density_Score"] = (schools - schools.min()) / (schools.max() - schools.min() + 1e-9)
    print("School_Density_Score created.")
else:
    print("Nearby_Schools column not found.")

print(df[["Price_per_SqFt", "School_Density_Score"]].head())

# Label based on Price vs City median
if {"City", "Price_in_Lakhs"}.issubset(df.columns):
    city_median_price = df.groupby("City")["Price_in_Lakhs"].transform("median")
    df["Good_Investment_price"] = (df["Price_in_Lakhs"] <= city_median_price).astype(int)
    print("Good_Investment_price created.")
else:
    print("City or Price_in_Lakhs column missing for price-based label.")

# 4.2 Label based on Price_per_SqFt vs City median
if {"City", "Price_per_SqFt"}.issubset(df.columns):
    city_median_pps = df.groupby("City")["Price_per_SqFt"].transform("median")
    df["Good_Investment_pps"] = (df["Price_per_SqFt"] <= city_median_pps).astype(int)
    print("Good_Investment_pps created.")
else:
    print("City or Price_per_SqFt column missing for pps-based label.")

# 4.3 Multi-factor score (edit columns and conditions as per your data)
df["investment_score"] = 0

# BHK condition
if "BHK" in df.columns:
    df["investment_score"] += np.where(df["BHK"] >= 3, 1, 0)

# Example RERA column (binary 0/1) if present
if "RERA" in df.columns:
    df["investment_score"] += np.where(df["RERA"] == 1, 1, 0)

# Ready-to-move via Availability_Status (if still present as encoded text in another version)
# Note: 'Availability_Status' column was one-hot encoded and removed from df.
# If the intention is to use the original string column, the encoding step
# needs to be adjusted not to drop it from df before this step.
if "Availability_Status" in df.columns:
    df["investment_score"] += np.where(
        df["Availability_Status"].str.contains("Ready", case=False, na=False),
        1,
        0
    )

# School density high
if "School_Density_Score" in df.columns:
    df["investment_score"] += np.where(df["School_Density_Score"] > 0.5, 1, 0)

# Threshold for Good Investment
df["Good_Investment_multi"] = (df["investment_score"] >= 2).astype(int)

print(df[["investment_score", "Good_Investment_price", "Good_Investment_pps", "Good_Investment_multi"]].head())
print("Value counts (multi-factor):")
print(df["Good_Investment_multi"].value_counts())

import matplotlib.pyplot as plt
import seaborn as sns

# EDA
# Price trends by city
# Option A: Average price per city (barplot)
plt.figure(figsize=(12, 6))
city_price = df.groupby("City")["Price_in_Lakhs"].mean().sort_values(ascending=False).head(20)
sns.barplot(x=city_price.index, y=city_price.values)
plt.xticks(rotation=75, ha="right")
plt.ylabel("Average Price (Lakhs)")
plt.title("Average Property Price by City")
plt.tight_layout()
plt.show()

# Option B: Boxplot of price distribution by top cities
top_cities = df["City"].value_counts().head(10).index
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[df["City"].isin(top_cities)], x="City", y="Price_in_Lakhs")
plt.xticks(rotation=75, ha="right")
plt.ylabel("Price (Lakhs)")
plt.title("Price Distribution for Top 10 Cities")
plt.tight_layout()
plt.show()

# Correlation between area and investment return

# If you have a numeric investment return column (e.g., Investment_Return or Appreciation_Rate):

# Scatter + correlation
if {"Size_in_SqFt", "Investment_Return"}.issubset(df.columns):
    plt.figure(figsize=(7, 5))
    sns.scatterplot(data=df, x="Size_in_SqFt", y="Investment_Return", alpha=0.4)
    plt.xlabel("Size (SqFt)")
    plt.ylabel("Investment Return")
    plt.title("Area vs Investment Return")
    plt.tight_layout()
    plt.show()

    corr = df[["Size_in_SqFt", "Investment_Return"]].corr().iloc
    print("Correlation between Size_in_SqFt and Investment_Return:", corr)
else:
    print("Columns 'Size_in_SqFt' or 'Investment_Return' not found. Create Investment_Return first if needed.")

# If you are using Future_Price_5Y and current price to approximate return:

if {"Price_in_Lakhs", "Future_Price_5Y", "Size_in_SqFt"}.issubset(df.columns):
    df["Investment_Return"] = (df["Future_Price_5Y"] - df["Price_in_Lakhs"]) / df["Price_in_Lakhs"]
    plt.figure(figsize=(7, 5))
    sns.scatterplot(data=df, x="Size_in_SqFt", y="Investment_Return", alpha=0.4)
    plt.xlabel("Size (SqFt)")
    plt.ylabel("Investment Return (Future vs Current)")
    plt.title("Area vs Investment Return")
    plt.tight_layout()
    plt.show()

    corr = df[["Size_in_SqFt", "Investment_Return"]].corr().iloc
    print("Correlation between Size_in_SqFt and Investment_Return:", corr)
else:
    print("Need Price_in_Lakhs, Future_Price_5Y, and Size_in_SqFt columns.")

# Impact of crime rate on good investment classification

# Assume you have a Crime_Rate column (e.g., crimes per lakh population for that city/locality).

label_col = "Good_Investment_multi" # or Good_Investment_price / Good_Investment_pps

if {"Crime_Rate", label_col}.issubset(df.columns):
    # Compare distributions
    plt.figure(figsize=(7, 5))
    sns.boxplot(data=df, x=label_col, y="Crime_Rate")
    plt.xticks([0, 1], ["Not Good Investment", "Good Investment"])
    plt.ylabel("Crime Rate")
    plt.title("Crime Rate vs Good Investment Classification")
    plt.tight_layout()
    plt.show()

    # Grouped stats
    stats = df.groupby(label_col)["Crime_Rate"].describe()
    print(stats)
else:
    print("Crime_Rate or label column not found. Make sure Crime_Rate and Good_Investment_* exist.")

# Relationship between infrastructure score and resale value

# Assume:

# Infrastructure_Score is a feature you built (0-1 or 0-10) combining schools, hospitals, transport, amenities.

# Resale value ≈ Future_Price_5Y or price in lakhs for resale listings.

# Option A: Using Infrastructure_Score and Future_Price_5Y

if {"Infrastructure_Score", "Future_Price_5Y"}.issubset(df.columns):
    plt.figure(figsize=(7, 5))
    sns.scatterplot(data=df, x="Infrastructure_Score", y="Future_Price_5Y", alpha=0.4)
    plt.xlabel("Infrastructure Score")
    plt.ylabel("Future Price (5Y) in Lakhs")
    plt.title("Infrastructure Score vs Future Resale Value")
    plt.tight_layout()
    plt.show()

    corr = df[["Infrastructure_Score", "Future_Price_5Y"]].corr().iloc
    print("Correlation between Infrastructure_Score and Future_Price_5Y:", corr)
else:
    print("Infrastructure_Score or Future_Price_5Y not found.")

# Option B: Using current price as resale proxy

if {"Infrastructure_Score", "Price_in_Lakhs"}.issubset(df.columns):
    plt.figure(figsize=(7, 5))
    sns.scatterplot(data=df, x="Infrastructure_Score", y="Price_in_Lakhs", alpha=0.4)
    plt.xlabel("Infrastructure Score")
    plt.ylabel("Current Price (Lakhs)")
    plt.title("Infrastructure Score vs Current Resale Value")
    plt.tight_layout()
    plt.show()

    corr2 = df[["Infrastructure_Score", "Price_in_Lakhs"]].corr().iloc
    print("Correlation between Infrastructure_Score and Price_in_Lakhs:", corr2)
else:
    print("Infrastructure_Score or Price_in_Lakhs not found.")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# The df DataFrame already contains the labeled data from previous steps
# print("Shape:", df.shape)

# Select Good Investment target (choose one)
target_class = "Good_Investment_multi"  # or Good_Investment_price, Good_Investment_pps

# Create Future_Price_5Y (8% annual appreciation x 5 years = ~47% total growth)
if "Price_in_Lakhs" in df.columns:
    df["Future_Price_5Y"] = df["Price_in_Lakhs"] * 1.47  # (1+0.08)^5 ≈ 1.47
    print("Future_Price_5Y created with 8% annual growth.")
else:
    print("Price_in_Lakhs not found. Cannot create Future_Price_5Y.")

print(f"Class distribution ({target_class}):")
print(df[target_class].value_counts(normalize=True))

# --- Start of the fix: Ensure all features for X are numerical ---
# We will create a new DataFrame `X_preprocessed` that contains all features
# in their numerical or one-hot encoded format.

X_preprocessed = df.copy() # Start with the full df containing all engineered features

# Identify all categorical columns that are currently strings and need encoding.
cols_to_onehot = X_preprocessed.select_dtypes(include=['object', 'category']).columns.tolist()

if cols_to_onehot:
    encoder_all_cats = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    encoded_array_all_cats = encoder_all_cats.fit_transform(X_preprocessed[cols_to_onehot])
    encoded_cols_all_cats = encoder_all_cats.get_feature_names_out(cols_to_onehot)

    encoded_df_all_cats = pd.DataFrame(encoded_array_all_cats, columns=encoded_cols_all_cats, index=X_preprocessed.index)

    # Drop original categorical columns and concatenate encoded
    X_preprocessed = pd.concat([X_preprocessed.drop(columns=cols_to_onehot), encoded_df_all_cats], axis=1)

# Now, define X and y from the fully preprocessed DataFrame
y_class = X_preprocessed[target_class]
y_reg = X_preprocessed["Future_Price_5Y"]

# Define features (exclude ID, targets, etc.)
exclude_cols = ["ID", target_class, "Future_Price_5Y", "investment_score"]
X = X_preprocessed.drop(columns=exclude_cols, errors='ignore')

# --- Additional defensive step: Ensure X is fully numeric before splitting ---
# Force conversion of all columns to numeric where possible
for col in X.columns:
    # Try to convert to numeric, coerce errors will turn non-convertible values into NaN
    X[col] = pd.to_numeric(X[col], errors='coerce')

# After conversion, fill any NaNs that might have been introduced by 'coerce' or existed previously
# Using median for numerical columns, or 0 if median is not suitable for all columns
for col in X.columns:
    if X[col].isnull().any():
        if pd.api.types.is_numeric_dtype(X[col]):
            median_val = X[col].median()
            X[col] = X[col].fillna(median_val)
        else: # Should not happen if previous step worked, but for safety
            X[col] = X[col].fillna(0) # or drop column if it's consistently non-numeric

# Final check: Drop any columns that are still non-numeric (e.g., if pd.to_numeric failed completely for a column)
non_numeric_cols_in_X = X.select_dtypes(include=['object', 'category']).columns
if not non_numeric_cols_in_X.empty:
    print(f"Warning: Dropping non-numeric columns from X before splitting: {list(non_numeric_cols_in_X)}")
    X = X.drop(columns=non_numeric_cols_in_X)

print("Features dtypes before split:")
print(X.dtypes.value_counts())

# --- End of additional defensive step ---

print("Features shape:", X.shape)
print("Classification target shape:", y_class.shape)
print("Regression target shape:", y_reg.shape)

# Train-test split (same splits for both tasks)
X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(
    X, y_class, y_reg, test_size=0.2, random_state=42, stratify=y_class
)

print("Train shapes:", X_train.shape, y_class_train.shape, y_reg_train.shape)
print("Test shapes:", X_test.shape, y_class_test.shape, y_reg_test.shape)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# The df DataFrame already contains the labeled data from previous steps
# df = pd.read_csv("india_housing_prices_labeled.csv") # This line is removed

target_class = "Good_Investment_multi"  # or Good_Investment_price / Good_Investment_pps

# Create Future_Price_5Y if not present
if "Future_Price_5Y" not in df.columns and "Price_in_Lakhs" in df.columns:
    df["Future_Price_5Y"] = df["Price_in_Lakhs"] * 1.47  # 8% annual growth for 5 years

# Drop rows with missing target
df = df.dropna(subset=[target_class, "Future_Price_5Y"])

# Define features
exclude_cols = ["ID", target_class, "Future_Price_5Y", "investment_score"]
feature_cols = [c for c in df.columns if c not in exclude_cols]

X = df[feature_cols]
y_class = df[target_class]
y_reg = df["Future_Price_5Y"]

# Identify numeric and categorical columns
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()

print("Numeric features:", num_cols)
print("Categorical features:", cat_cols)

X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(
    X, y_class, y_reg, test_size=0.2, random_state=42, stratify=y_class
)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessor: one-hot for categorical, passthrough numeric
preprocessor = ColumnTransformer(
    transformers=[
        ("num", "passthrough", num_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
    ]
)

models_class = {
    "Logistic Regression": LogisticRegression(random_state=42, max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(
        random_state=42,
        eval_metric="logloss",
        use_label_encoder=False
    ),
}

results_class = {}

for name, clf in models_class.items():
    print(f"\nTraining {name}...")
    pipe = Pipeline(steps=[("preprocess", preprocessor),
                           ("model", clf)])

    pipe.fit(X_train, y_class_train)

    y_pred = pipe.predict(X_test)
    y_proba = pipe.predict_proba(X_test)[:, 1]

    results_class[name] = {
        "Accuracy": accuracy_score(y_class_test, y_pred),
        "Precision": precision_score(y_class_test, y_pred),
        "Recall": recall_score(y_class_test, y_pred),
        "F1": f1_score(y_class_test, y_pred),
        "ROC_AUC": roc_auc_score(y_class_test, y_proba),
        "pipeline": pipe,
    }

    print(f"{name} - Acc: {results_class[name]['Accuracy']:.3f}, "
          f"F1: {results_class[name]['F1']:.3f}, "
          f"AUC: {results_class[name]['ROC_AUC']:.3f}")

# Summary
results_df_class = pd.DataFrame({
    k: {m: v for m, v in results_class[k].items() if m != "pipeline"}
    for k in results_class
}).T
print("\nClassification results:\n", results_df_class.round(3))

# Best classification model feature importance
best_class_model_name = max(results_class, key=lambda x: results_class[x]['ROC_AUC'])
print(f"\nBest Classification Model: {best_class_model_name}")

# Get the pipeline for the best classification model
best_clf_pipeline = results_class[best_class_model_name]['pipeline']

# Check if the model in the pipeline has feature_importances_
# The actual model is the last step of the pipeline
clf_model = best_clf_pipeline.named_steps['model']

if hasattr(clf_model, 'feature_importances_'):
    importances = clf_model.feature_importances_

    # Get feature names after preprocessing
    preprocessor_step = best_clf_pipeline.named_steps['preprocess']
    feature_names_after_preprocessing = preprocessor_step.get_feature_names_out()

    feat_imp_df = pd.DataFrame({
        'feature': feature_names_after_preprocessing,
        'importance': importances
    }).sort_values('importance', ascending=False).head(10)

    plt.figure(figsize=(10, 6))
    sns.barplot(data=feat_imp_df, x='importance', y='feature')
    plt.title(f'Top 10 Features - {best_class_model_name}')
    plt.tight_layout()
    plt.show()

    print(feat_imp_df)

# Best regression model feature importance
# This part assumes 'results_reg' and 'models_reg' exist, which they do not in the current state
# I will comment out this part to prevent further errors for now.
# If regression models were trained, similar logic would apply.
# best_reg_model = min(results_reg, key=lambda x: results_reg[x]['RMSE'])
# print(f"\nBest Regression Model: {best_reg_model}")

# if hasattr(models_reg[best_reg_model], 'feature_importances_'):
#     importances_reg = models_reg[best_reg_model].feature_importances_
#     feat_imp_reg_df = pd.DataFrame({
#         'feature': feature_cols,
#         'importance': importances_reg
#     }).sort_values('importance', ascending=False).head(10)

#     plt.figure(figsize=(10, 6))
#     sns.barplot(data=feat_imp_reg_df, x='importance', y='feature')
#     plt.title(f'Top 10 Features - {best_reg_model}')
#     plt.tight_layout()
#     plt.show()

#     print(feat_imp_reg_df)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Get the best classification model's pipeline (assuming 'best_class_model_name' is available from previous cell)
best_class_model_name = max(results_class, key=lambda x: results_class[x]['ROC_AUC'])
best_clf_pipeline = results_class[best_class_model_name]['pipeline']

# Make predictions using the best model on the test set
y_class_pred = best_clf_pipeline.predict(X_test)
y_class_proba = best_clf_pipeline.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_class_test, y_class_pred)
precision = precision_score(y_class_test, y_class_pred)
recall = recall_score(y_class_test, y_class_pred)
f1 = f1_score(y_class_test, y_class_pred)
roc_auc = roc_auc_score(y_class_test, y_class_proba)

print(f"Accuracy : {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall   : {recall:.3f}")
print(f"F1-score : {f1:.3f}")
print(f"ROC AUC  : {roc_auc:.3f}")

!pip install mlflow==2.16.0

import mlflow
import mlflow.sklearn

# Store runs in local folder inside Colab
mlflow.set_tracking_uri("file:///content/mlruns")

# Create / set experiment
mlflow.set_experiment("real_estate_investment")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import mlflow
import mlflow.sklearn

def run_classification_mlflow(model_name, model_pipeline, X_train_raw, X_test_raw, y_train, y_test):
    with mlflow.start_run(run_name=f"cls_{model_name}"):
        # Log model parameters from the pipeline's final estimator
        model_component = model_pipeline.named_steps['model']
        model_params = model_component.get_params()
        mlflow.log_params(model_params)

        # Train the pipeline
        model_pipeline.fit(X_train_raw, y_train)

        # Predict using the pipeline
        y_pred = model_pipeline.predict(X_test_raw)
        y_proba = model_pipeline.predict_proba(X_test_raw)[:, 1]

        # Metrics
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred)
        rec = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_proba)

        mlflow.log_metric("accuracy", acc)
        mlflow.log_metric("precision", prec)
        mlflow.log_metric("recall", rec)
        mlflow.log_metric("f1", f1)
        mlflow.log_metric("roc_auc", auc)

        # Log model pipeline
        mlflow.sklearn.log_model(model_pipeline, artifact_path="model")

        print(f"{model_name}: acc={acc:.3f}, f1={f1:.3f}, auc={auc:.3f}")

# Check if results_class is defined in the global scope
if 'results_class' not in globals():
    print("Error: 'results_class' is not defined. Please ensure the preceding cells, "
          "especially the one where classification models are trained (e.g., Bu3mqHsG-lrS), "
          "have been executed successfully before running this cell.")
else:
    # Get the pipelines from the results_class dictionary
    log_reg_pipeline = results_class["Logistic Regression"]["pipeline"]
    rf_pipeline = results_class["Random Forest"]["pipeline"]
    xgb_pipeline = results_class["XGBoost"]["pipeline"]

    # Call run_classification_mlflow for each pipeline, passing the raw X_train/X_test
    # The pipeline itself handles the preprocessing.
    run_classification_mlflow("Logistic Regression", log_reg_pipeline, X_train, X_test, y_class_train, y_class_test)
    run_classification_mlflow("Random Forest", rf_pipeline, X_train, X_test, y_class_train, y_class_test)
    run_classification_mlflow("XGBoost", xgb_pipeline, X_train, X_test, y_class_train, y_class_test)